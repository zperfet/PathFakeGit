# ä¸»è¦ç”¨äºphemeæ•°æ®çš„é¢„å¤„ç†ï¼›å¯¹å·²æœ‰æ•°æ®è¿›è¡Œä¸€å®šçš„æ•´åˆå’Œè½¬æ¢
# å› ä¸ºåŸå§‹æ•°æ®å’Œéƒ¨åˆ†è®¾ç½®ä¸¢å¤±ï¼Œåªå‰©ä¸‹äº†å¤„ç†åçš„phemeæ•°æ®
# å¦‚æœéœ€è¦è·å–phemeæºæ•°æ®ï¼Œæ³¨æ„æ­¤å¤„å¤„ç†çš„æ˜¯5ç±»äº‹ä»¶çš„phemeæ•°æ®
from config import *
from data_io import *
from treelib import Tree
from preprocess4wae import *
import re
from get_args import _args


# æ ¹æ®æ ‘æ–‡ä»¶å¤¹ç”Ÿæˆè·¯å¾„idæ–‡ä»¶å’Œè·¯å¾„æ–‡æœ¬æ–‡ä»¶
# è¾“å…¥:æ ‘æ–‡ä»¶å¤¹,æ¯æ£µæ ‘å¯¹åº”ä¸€ä¸ªæ–‡ä»¶,æ–‡ä»¶ä¸­æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€å¯¹çˆ¶å­èŠ‚ç‚¹,ä»¥åŠèŠ‚ç‚¹çš„æ—¶é—´
# è¾“å‡º:
# 1)æ ¹æ®æ ‘æ–‡ä»¶ç”Ÿæˆçš„è·¯å¾„id,æ¯ä¸€è¡Œå¯¹åº”ä¸€æ¡è·¯å¾„ä¸­å…¨éƒ¨èŠ‚ç‚¹çš„id,ä»æ ¹èŠ‚ç‚¹åˆ°å¶å­èŠ‚ç‚¹
# 2)æ ¹æ®æ ‘æ–‡ä»¶ç”Ÿæˆçš„è·¯å¾„text,æ¯ä¸€è¡Œå¯¹åº”ä¸€æ¡è·¯å¾„ä¸­çš„å…¨éƒ¨æ–‡æœ¬,æ–‡æœ¬é¡ºåºå’Œidå¯¹åº”
def generate_path_ids_and_texts(tree_dir_path, path_ids_save_path, path_texts_save_path):
    names = os.listdir(tree_dir_path)
    id2text_dict = get_text_dict(pheme_all_text_path)
    path_sum = 0
    all_path_ids = []
    all_path_texts = []
    for cnt, name in enumerate(names):
        path = join(tree_dir_path, name)
        source_id = name.split('.')[0]
        with open(path, 'r')as f:
            lines = [line.strip() for line in f.readlines()]
        tree = Tree()
        tree.create_node(identifier=source_id)
        if lines[0] != '':
            for line in lines:
                line = line.split('\t')
                p_node, c_node = line[0], line[2]
                try:
                    tree.create_node(identifier=c_node, parent=p_node)
                except Exception:
                    pass
        # æ ¹æ®æ ‘ç”Ÿæˆè·¯å¾„
        path_ids = tree.paths_to_leaves()
        all_path_ids += ['\t'.join(line) for line in path_ids]
        # è·å–æ¯ä¸€æ¡è·¯å¾„å¯¹åº”æºæ–‡æœ¬çš„æ ‡ç­¾
        # path_label = [id2label_dict[line[0]] for line in path_ids]
        path_texts = [[id2text_dict[i] for i in line] for line in path_ids]
        path_texts = ['\t'.join(line) for line in path_texts]
        all_path_texts += path_texts
        # for i in range(len(path_ids)):
        #     all_path_texts.append(path_texts[i] + '\t' + path_label[i])
        path_sum += len(path_ids)
    with open(path_ids_save_path, 'w')as f:
        f.write('\n'.join(all_path_ids) + '\n')
    with open(path_texts_save_path, 'w')as f:
        f.write('\n'.join(all_path_texts) + '\n')


# æ ¹æ®æ ‘æ–‡ä»¶å¤¹ç”Ÿæˆè·¯å¾„idæ–‡ä»¶å’Œè·¯å¾„æ–‡æœ¬æ–‡ä»¶
# è¾“å…¥:æ ‘æ–‡ä»¶å¤¹,æ¯æ£µæ ‘å¯¹åº”ä¸€ä¸ªæ–‡ä»¶,æ–‡ä»¶ä¸­æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€å¯¹çˆ¶å­èŠ‚ç‚¹,ä»¥åŠèŠ‚ç‚¹çš„æ—¶é—´
# è¾“å‡º:
# 1)æ ¹æ®æ ‘æ–‡ä»¶ç”Ÿæˆçš„è·¯å¾„id,æ¯ä¸€è¡Œå¯¹åº”ä¸€æ¡è·¯å¾„ä¸­å…¨éƒ¨èŠ‚ç‚¹çš„id,ä»æ ¹èŠ‚ç‚¹åˆ°å¶å­èŠ‚ç‚¹
# 2)æ ¹æ®æ ‘æ–‡ä»¶ç”Ÿæˆçš„è·¯å¾„text,æ¯ä¸€è¡Œå¯¹åº”ä¸€æ¡è·¯å¾„ä¸­çš„å…¨éƒ¨æ–‡æœ¬,æ–‡æœ¬é¡ºåºå’Œidå¯¹åº”
def generate_path_ids_and_texts_early(tree_dir_path, max_time, save_path):
    names = os.listdir(tree_dir_path)
    tree_ids_dict = dict()
    path_cnt = 0
    node_cnt = 0
    for cnt, name in enumerate(names):
        path = join(tree_dir_path, name)
        source_id = name.split('.')[0]
        tree_ids_dict[source_id] = []
        with open(path, 'r')as f:
            lines = [line.strip() for line in f.readlines()]
        # å°†è¶…è¿‡max-timeçš„è¡Œåˆ é™¤
        lines = [line.split('\t') for line in lines]
        lines = [line for line in lines if len(line) == 4 and
                 float(line[1]) <= max_time and float(line[3]) <= max_time]
        lines = ['\t'.join(line) for line in lines]
        if len(lines) == 0: continue
        tree = Tree()
        tree.create_node(identifier=source_id)
        if lines[0] != '':
            for line in lines:
                line = line.split('\t')
                p_node, c_node = line[0], line[2]
                try:
                    tree.create_node(identifier=c_node, parent=p_node)
                except Exception:
                    pass
        # æ ¹æ®æ ‘ç”Ÿæˆè·¯å¾„
        path_ids = tree.paths_to_leaves()
        # è·å–æ¯ä¸€æ¡è·¯å¾„å¯¹åº”æºæ–‡æœ¬çš„æ ‡ç­¾
        # path_texts = [[id2text_dict[i] for i in line] for line in path_ids]
        # path_texts = ['\t'.join(line) for line in path_texts]
        # tree_ids_dict[source_id] += path_texts
        tree_ids_dict[source_id] = path_ids
        path_cnt += len(path_ids)
        node_cnt += len(tree.nodes.keys())
    print("max_time:", max_time, "path_nums:", path_cnt, "node_nums:", node_cnt)
    # print("save dict to", save_path)
    save_json_dict(tree_ids_dict, save_path)


# æ¶ˆé™¤è·¯å¾„æ–‡æœ¬ä¸­çš„æºæ–‡æœ¬ï¼›åŸºäºå›å¤æ–‡æœ¬æ„å»ºè·¯å¾„
# è¾“å…¥:
# 1ï¼‰è·¯å¾„idæ–‡ä»¶è·¯å¾„ï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€æ¡è·¯å¾„ï¼Œè·¯å¾„çš„ç¬¬ä¸€ä¸ªidæ˜¯æºæ–‡æœ¬id
# 2ï¼‰idã€æ–‡æœ¬å¯¹åº”çš„æ–‡æœ¬è·¯å¾„
# è¾“å‡ºï¼šè·¯å¾„idå¯¹åº”çš„æ–‡æœ¬ï¼Œå…¶ä¸­æºæ–‡æœ¬idæ¶ˆé™¤äº†æºæ–‡æœ¬
def get_response_path(path_ids_path, save_path):
    # è¯»å–path-idsï¼Œå¹¶åˆ é™¤æºæ–‡æœ¬çš„id
    with open(path_ids_path, 'r')as f:
        response_path_ids = [line.strip().split('\t')[1:] for line in f.readlines()]

    # è·å–dict(id:text)
    id2text_dict = get_text_dict(pheme_all_text_path)

    response_path_texts = []
    for id_lst in response_path_ids:
        tmp_path_text = []
        for id in id_lst:
            tmp_path_text.append(id2text_dict[id])
        response_path_texts.append('\t'.join(tmp_path_text))
    with open(save_path, 'w')as f:
        f.write('\n'.join(response_path_texts) + '\n')


# æ ¹æ®ç»™å®šçš„æ¨ç‰¹idè¿”å› åˆ†è¯åæ ¹æ®è¯æ±‡è¡¨æ˜ å°„å¾—åˆ°çš„idåˆ—è¡¨
# token2id_dict_path:è¯æ±‡è¡¨å¯¹åº”è·¯å¾„,æ ¹æ®è¯æ±‡è¡¨è¿”å›å¯¹åº”idï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›0
# tweet_id:æ¨ç‰¹æ–‡æœ¬å¯¹åº”çš„id,æ­¤å¤„ä¸ºtwitter15|16æ•°æ®é›†å¯¹åº”çš„å…¨éƒ¨ids;æ³¨æ„ä¸åŒæ•°æ®é›†å¯¹åº”çš„è¯æ±‡è¡¨ä¸åŒ
# save_path:ä¿å­˜tweet_idï¼štweet_token_idså­—å…¸
def tweet_id2_token_ids(vocab_path, tweet_id_lst, save_path):
    # è¯»å–è¯æ±‡å¹¶ç”Ÿæˆå­—å…¸
    with open(vocab_path, 'r')as f:
        tokens = [token.strip() for token in f.readlines()]
    token2id_dict = dict()
    for cnt, token in enumerate(tokens):
        token2id_dict[token] = cnt
    tokens_set = set(token2id_dict.keys())
    # åŠ è½½id->textå­—å…¸
    id2text_dict = get_text_dict()
    # åˆ†è¯
    nlp = spacy.load('en_core_web_sm')
    tokenizer = Tokenizer(nlp.vocab)
    res_dict = dict()
    PATTERN = re.compile(r'\b[^\d\W]{2,20}\b')
    for cnt, tid in enumerate(tweet_id_lst):
        if (cnt + 1) % 1000 == 0:
            print("å¤„ç†", cnt + 1)
        # æ ¹æ®idè·å–æ–‡æœ¬
        texts = id2text_dict[tid]
        # æ–‡æœ¬é¢„å¤„ç†
        texts = texts.lower()
        # texts = PATTERN.sub(" ", texts)
        texts = ''.join(c if c.isalpha() else ' ' for c in texts)
        texts = emoji_replace(texts)
        texts = delete_special_freq(texts)
        line_tokens = list(map(str, tokenizer(texts)))
        line_ids = [token2id_dict[word] for word in line_tokens if word in tokens_set]
        line_ids = sorted(set(line_ids))
        res_dict[tid] = line_ids
    # å°†è¯å…¸ä¿å­˜åˆ°æœ¬åœ°
    print('å°†è¯å…¸ä¿å­˜åˆ°æœ¬åœ°')
    save_json_dict(res_dict, save_path)


# ä½¿ç”¨ç‰¹å®šå­—ç¬¦æ›¿ä»£ç‰¹æ®Šå­—ç¬¦
# 1ï¼‰è¡¨æƒ…ï¼š
# ğŸ˜‚1281;ğŸ˜³417;ğŸ‘€68;ğŸ™73;â¤85;ğŸ‘98;ğŸ™Œ101;ğŸ˜­521;
# ğŸ˜’77;ğŸ˜©173;ğŸ˜·101;ğŸ‘84;ğŸ˜158;ğŸ‰66;ğŸ˜«50;ğŸ˜”103;
# ğŸ’”88;ğŸ‘56;ğŸ˜Š49;ğŸ˜44;ğŸ™…48
def emoji_replace(text):
    emoji_replace_dict = {"ğŸ˜‚": " emoji_a ", "ğŸ˜³": " emoji_b ", "ğŸ‘€": " emoji_c ",
                          "ğŸ™": " emoji_d ", "â¤": " emoji_e ", "ğŸ‘": " emoji_f ",
                          "ğŸ™Œ": " emoji_g ", "ğŸ˜­": " emoji_h ", "ğŸ˜’": " emoji_i ",
                          "ğŸ˜©": " emoji_j ", "ğŸ˜·": " emoji_k ", "ğŸ‘": " emoji_l ",
                          "ğŸ˜": " emoji_m ", "ğŸ‰": " emoji_n ", "ğŸ˜«": " emoji_o ",
                          "ğŸ˜”": " emoji_p ", "ğŸ’”": " emoji_q ", "ğŸ˜Š": " emoji_r ",
                          "ğŸ˜": " emoji_s ", "ğŸ™…": " emoji_t "}
    for k in emoji_replace_dict.keys():
        if k in text:
            text = text.replace(k, emoji_replace_dict[k])
    return text


# åˆ é™¤å¥å­ä¸­ç±»ä¼¼@å’Œurl
def delete_special_freq(text):
    if len(text) < 20: return text
    # print('è½¬æ¢å‰ï¼š', text)
    raw = text
    text = [word for word in text.split() if word[0] != '@']
    text = ' '.join(text)
    # å¦‚æœåªæœ‰@ï¼Œåˆ™ä¿ç•™@
    if text == '': text = raw
    text = text.replace('URL', '')
    # å¦‚æœåªæœ‰urlï¼Œåˆ™ä¿ç•™url
    if text == '': text = raw
    # print('è½¬æ¢åï¼š', text)
    # print()
    return text


# ä½¿ç”¨TOKENæ›¿æ¢urlä¸­çš„é“¾æ¥
def replace_url_with_token(url):
    # print("æ›¿æ¢å‰ï¼š", url)
    url = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', 'URL', url, flags=re.MULTILINE)
    # print("æ›¿æ¢åï¼š", url)
    return url


# early detectioné¢„å¤„ç†
# å‚æ•°ï¼š
# 1ï¼‰æ ‘æ–‡ä»¶å¤¹,æ¯æ£µæ ‘å¯¹åº”ä¸€ä¸ªæ–‡ä»¶,æ–‡ä»¶ä¸­æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€å¯¹çˆ¶å­èŠ‚ç‚¹,ä»¥åŠèŠ‚ç‚¹çš„æ—¶é—´
# 2ï¼‰ä¿å­˜é¢„å¤„ç†æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
# 3ï¼‰æœ€è¿Ÿçš„æ—¶é—´æ®µ
def early_detection_truncation():
    time_interval = [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 120]
    # time_interval = [100000000]
    tree15_dir_path = join(data_dir_path, 'TreeTwitter15')
    tree16_dir_path = join(data_dir_path, 'TreeTwitter16')
    for max_time in time_interval:
        save15_path = join(early_detection_dir_path,
                           "early_twitter15_interval%d.json" % max_time)
        save16_path = join(early_detection_dir_path,
                           "early_twitter16_interval%d.json" % max_time)
        generate_path_ids_and_texts_early(tree15_dir_path, max_time, save15_path)
        generate_path_ids_and_texts_early(tree16_dir_path, max_time, save16_path)


# åˆ’åˆ†æ•°æ®ï¼Œè·å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†
# è¾“å…¥ï¼šfold_xè¡¨ç¤ºæ˜¯äº¤å‰éªŒè¯çš„ç¬¬å‡ ä¸ªï¼Œfold_xæ˜¯0-4ä¹‹é—´çš„æ•°å­—
# è¾“å‡ºï¼šåˆ’åˆ†åçš„è®­ç»ƒã€æµ‹è¯•é›†ï¼Œæ¯ä¸€è¡Œä¸€ä¸ªsource id
def dataset_split(fold_x):
    fold_train_path = join(pheme_fold_dir_path, 'pheme5_train%d_ids.txt' % fold_x)
    fold_test_path = join(pheme_fold_dir_path, 'pheme5_test%d_ids.txt' % fold_x)
    train_ids = load_lst(fold_train_path)
    test_ids = load_lst(fold_test_path)
    random.shuffle(train_ids)
    random.shuffle(test_ids)
    return train_ids, test_ids


if __name__ == '__main__':
    # early_detection_truncation()
    pheme_source_path = join(data_dir_path, 'pheme_source.txt')
    pheme_response_path = join(data_dir_path, 'pheme_response.txt')
    pheme_all_text_path = join(data_dir_path, 'pheme_all_text.txt')
    # path_texts_raw_path=join(data_dir_path,'pheme_all_path_texts.txt')

    pheme_source_lst = get_tweet_id_lst(pheme_source_path)
    pheme_response_lst = get_tweet_id_lst(pheme_response_path)

    # random_pheme_vocab4random_path = join(data_dir_path, 'random_pheme_vocab4random')
    # response_pheme_vocab4wae_path = join(data_dir_path, 'response_pheme_vocab4wae')

    # random_pheme_vocab_path = join(random_pheme_vocab4random_path, 'vocabulary', 'vocab.txt')
    # response_pheme_vocab_path = join(response_pheme_vocab4wae_path, 'vocabulary', 'vocab.txt')

    # tweet_id2_token_ids(random_pheme_vocab_path, pheme_source_lst + pheme_response_lst,
    #                     join(data_dir_path, "early_random15_ids.json"))
    # tweet_id2_token_ids(response_pheme_vocab_path, pheme_source_lst + pheme_response_lst,
    #                     join(data_dir_path, "early_response15_ids.json"))

    # åˆå¹¶å…¨éƒ¨phemeæ–‡æœ¬
    pheme_source_lines = load_lst(pheme_source_path)
    pheme_response_lines = load_lst(pheme_response_path)
    pheme_all_text = pheme_source_lines + pheme_response_lines
    with open(pheme_all_text_path, 'w')as f:
        f.write('\n'.join(pheme_all_text) + '\n')

    # æ ¹æ®æ ‘æ–‡ä»¶ç”Ÿæˆè·¯å¾„idså’Œè·¯å¾„texts
    print('æ ¹æ®æ ‘æ–‡ä»¶ç”Ÿæˆè·¯å¾„idså’Œè·¯å¾„texts...')
    generate_path_ids_and_texts(tree_dir_path, path_ids_path, path_texts_raw_path)

    # # æ ¹æ®è·¯å¾„idsç”Ÿæˆå›å¤è·¯å¾„texts
    print('æ ¹æ®è·¯å¾„idsç”Ÿæˆå›å¤ids...')
    get_response_path(path_ids_path, path_response_text_raw_path)

    # # é¢„å¤„ç†è·¯å¾„textsï¼šæ›¿æ¢è¡¨æƒ…åŒ…ï¼Œåˆ é™¤urlå’Œ@ç­‰ç¬¦å·
    print('é¢„å¤„ç†è·¯å¾„æ–‡æœ¬...')
    path_lines = load_lst(path_texts_raw_path)
    path_lines = [replace_url_with_token(line) for line in path_lines]
    path_lines = [emoji_replace(line) for line in path_lines]
    path_lines = [delete_special_freq(line) for line in path_lines]
    save_lst(path_texts_path, path_lines)
    #
    # # é¢„å¤„ç†å›å¤è·¯å¾„textsï¼šæ›¿æ¢è¡¨æƒ…åŒ…ï¼Œåˆ é™¤urlå’Œ@ç­‰ç¬¦å·
    print('é¢„å¤„ç†å›å¤è·¯å¾„æ–‡æœ¬...')
    response_lines = load_lst(path_response_text_raw_path)
    response_lines = [replace_url_with_token(line) for line in response_lines]
    response_lines = [emoji_replace(line) for line in response_lines]
    response_lines = [delete_special_freq(line) for line in response_lines]
    save_lst(path_response_text_path, response_lines)

    # ä¸ºWAEé¢„å¤„ç†è·¯å¾„æ–‡æœ¬å’Œå›å¤è·¯å¾„æ–‡æœ¬
    print('é¢„å¤„ç†pathæ–‡æœ¬...')
    pre4wae(path_vocab4random, path_texts_path, _args.random_vocab_dim - 1)
    print('é¢„å¤„ç†response pathæ–‡æœ¬...')
    pre4wae(response_path_vocab4wae, path_response_text_path, _args.response_vocab_dim - 1)

    # åˆ’åˆ†æ•°æ®é›†
    # pheme_train_ids, pheme_test_ids = dataset_split(_args.pheme_fold)
    # save_lst(pheme_train_ids_path, pheme_train_ids)
    # save_lst(pheme_test_ids_path, pheme_test_ids)
